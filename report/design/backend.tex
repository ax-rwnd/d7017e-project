% TODO move to a higher level (working methods?)
% A coding-standard was quickly decided on to make sure the code style was somewhat coherent effectively making it easier for different members to take over other members code if needed.

The backend of GPP was developed using javascript running with NodeJs as server framework and Express as a webframework to handle routing. NodeJs was chosen because it was the most established framework with wide support and multiple extensions, such as Express.

\subsection{Database} \label{database}

%OLD TEXT COMMENTED
%The document database MongoDB was chosen for the project instead of using a relational database, %the reasoning for this decision was to have a database that was built with scalability in mind %and dynamic schemas but also because of the simple reason that it is easy for the programmers to %read and understand the data inside the database thanks to MongoDBs JSON-like document %structure.
%OLD TEXT COMMENTED

MongoDB was chosen mainly because the project members aimed to use software that are included in the MEAN software stack (MongoDB, Express, Angular, NodeJS). The majority of the group members working with backend did not have prior experience working with NoSQL databases which means it also presented a good opportunity to learn. MongoDB was a popular choice among NoSQL databases and the group deemed it able to simplify the process of interacting with the data. Mongo uses a JSON-like document structure and JSON is the data format used when different parts of the project services communicates. 

It was discovered that a plug-in for MongoDB: Mongoose, would aid in the process of creating a database structure. With Mongoose it's possible to create schemas that describes the format of the data that is to be stored in the database. Using Mongoose did present a problem to the project members by not having consistency. If the schemas were updated there would be a mix of data, some of it stored in the previous format and some in the new. This resulted in a need to reset the database after schema updates.

While developing it also proved valuable to have a graphical representation of the data. The data is displayed using \href{https://github.com/mrvautin/adminMongo}{AdminMongo} which also enables the user to create new entries, edit existing data and delete data. Being able to insert and edit data presented some issues throughout the project since the user has full freedom when inserting data. No input validation or checks to the current database schemas are made. 


\subsection{API protocol}
A decision was made early in the project to adhere to a RESTful API design. Access paths to resources are built from the database schema\ref{https}. For example: in the current database design, each assignment belongs to a course. It is therefore necessary to reference a specific course to access an assignment. \texttt{Example: GET https://.../api/courses/123123123/assignments/456456456/} returns assignment \texttt{456456456} which belongs to course \texttt{123123123}. Each resource name such as \texttt{courses} is in plural. A POST to \texttt{https://.../api/courses} creates a course while a GET request to the same resource returns all courses.

Data can be sent to the backend via both body and URL. Sending data via body is relevant for PUT and POST requests. The information for such requests should be sent as JSON and it should match the format specified in the API documentation\ref{}. JSON is also the data-type used by the backend to return data, even when the result returned is a status code.

% errors
% extensibility in features/progress/gamification

\subsection{Security}
% login/tokens
% input validation
% access control

\subsection{HTTPS} \label{https}
Self-signed certificates have been used for the testing environment in order to establish a secure environment with HTTPS. Once a proper production environment is used, it is preferable to use a trusted certificate authority instead of self-signed certificates.

% SSL (HTTPS), just mention it
\subsubsection{Access control}
The API has three different base roles, 
\begin{itemize}
\item Basic
\item Advanced
\item Admin
\end{itemize}

These are assigned according to the user's role in their school, which is found when logging in through CAS. If the user is a student he will be assigned the basic role and if he is a teacher the advanced role is assigned. Admin is never assigned automatically and have to be set manually if needed. Depending on the role, different limitations are enforced. A basic user is only allowed to create 3 courses. Admins and advanced users are allowed to create an unlimited number of courses. This is the only difference between a basic and an advanced user. The reason a basic user is limited in their number of courses is simply to prevent malicious behaviour. Admins have full access to every route and are not limited in any way. This is mostly affecting course routes. 

Regardless of a user's base role he has a different role in every course. Every course in turn has three user roles,
\begin{itemize}
\item Student
\item Teacher
\item Owner
\end{itemize}
A student is only allowed to do things regarding himself, such as submitting assignments or leaving the course. The teacher role is to maintain the course but also to control who is allowed to join it. The highest course role is the owner which is the user that created the course. The owner can do everything a teacher is able to do, the only difference being that teachers can't remove the owner from the course. A user with the admin base role is basically able to do everything in any course.

\subsection{Quality control}
% jenkins
As a way to keep the project effective it was determined that it was important to implement continuous integration as a part of the project to have a modern workflow where you could push changes to git and have them automatically built on a live test server. Using continuous integration was also a good way to separate production and development builds and a way to ensure that production builds always kept a certain standard and robustness. The framework that was used for continuous integration was Jenkins which is one of the most popular frameworks.

While the idea of how continuous integration was supposed to be used in the project was quite clear, the concept wasn't put to as good use as it could have. In the first half of the project, the builds generated from Jenkins wasn't really put to use. This was mainly due to communication errors and the fact that the builds weren't needed as much. For the second part of the project the builds were used more. The way that the deployment with Jenkins was setup was to build from two branches from GIT, one from the master which was supposed to build for production and another from the backend branch which was supposed to build for development. The idea was that the development build would always contain the latest changes in backend. As the project reached its deadline it became increasingly important to also keep the backend dev builds stable because of the fact that the frontend team were depending on the backend dev build when developing.

Because of the increasing demands on having stable backend dev builds, a new requirement was setup for the continuous integration where a new development dev build was only built if it passed a set of unit-tests for the application.

% different environments
Connected to the reasoning of building the backend in different environments/modes such as production or development, it was important to load different configurations depending on what environment the backend was supposed to run. For example, the production environment should not use the same database as the development environment. Because of this, it was important to be able to run the backend in different modes and dynamically load the correct configurations based on the mode that the backend was running in, hence a system that did this was implemented.

% logging
A large part of developers and support teams work is monitoring, debugging and troubleshooting. Because of this it is important to have good logging as a way to make these processes easier, faster and smoother. Having good logging gives them a good insight into what the application is doing and how it works. Because of this, it was important to have proper, useful and extensible logging available in the backend. A custom logging system promtly named logger was developed that ran with Winston and Morgan at its core. Winston is a simple and universal logging library and Morgan is a HTTP request logger middleware. Logger combined winston and morgan with the selectiveness of running the backend in different modes/environments. Depending on the mode, logger had different parts that it logged into log files.

Logger was setup to log all errors, server errors and a general logs in separate log files. When running in dev mode it was also important to log to the console, but not otherwise, hence console logging was only enabled when the backend ran in dev mode. 

Winston and Logger use levels to identify what type of information a log contains, the logging levels in logger conform closely to the severity ordering specified by RFC6424.
The severity of all levels are numerically ascending from most important to least important.

\begin{enumerate}
  \item Server Error
  \item Error
  \item Warning
  \item Info
  \item Verbose
  \item Debug
  \item Silly
\end{enumerate}

% automated testing
The API is a large part of the backend codebase. Automated tests of the endpoints were developed, mainly to guard against regressions. The tests mostly assert that the server doesn't return an error when given good input. In order to save time in test development, the tests are run in order, where older tests create the resources that are needed for later tests. This is not good practice in automated tests, but it would have been far more costly to make the tests independent of each other by doing setup and teardown for each endpoint test.
